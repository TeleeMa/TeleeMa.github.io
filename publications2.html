<!-- <!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Publications</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head> -->

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="misc/UST.svg.png" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Teli Ma</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<!-- <div class="menu-item"><a href="experience.html">Experience</a></div>
<div class="menu-item"><a href="research.html">Research</a></div> -->
<!-- <div class="menu-item"><a href="awards.html">Awards</a></div> -->
<div class="menu-item"><a href="publications2.html" class="current">Publications</a></div>
<!-- <div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="activities.html">Activities</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Selected Publications</h1>
</div>
<!-- <p>In this page, name &ldquo;Teli Ma&rdquo; is shown in bold text and symbol &ldquo;*&rdquo; denotes the equal contribution.</p>
<p>The publications can also be found through <a href="https://scholar.google.com/citations?user=arny77IAAAAJ&hl=en&oi=ao">Google Scholar</a>.</p> -->
<ul>
<!-- <br /> -->

<body>
  <!-- <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px"> -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        </tbody></table>
        <table style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <heading>Research</heading>  -->
              <p> 
                In this page, name &ldquo;<b>Teli Ma</b>&rdquo; is shown in bold text and symbol &ldquo;<b>*</b>&rdquo; denotes the equal contribution. <br \>
                More publications can be found through <a href="https://scholar.google.com/citations?user=arny77IAAAAJ&hl=en&oi=ao">Google Scholar</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					
        <tr> 
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/track.gif"><img src="images/track.gif" alt="GLOVER" width="320" height="180"></a>
            </td>
            <td width="65%" valign="middle">
                <p> GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping <br />
                    <i><b>Teli Ma*</b>, Zifan Wang*, Jiaming Zhou, Mengmeng Wang, Junwei Liang</i><br />
                    arXiv preprint, 2024 <br />
                    [<a href="https://arxiv.org/pdf/2411.12286v1">paper</a>][<a href="https://teleema.github.io/projects/GLOVER/">homepage</a>]<br />
                    </p>
                    <br />
                  
            </td>
        </tr>
          

            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/mitigate.jpg"><img src="images/mitigate.jpg" alt="Mitigating" width="320" height="140"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation <br />
                        <i>Jiaming Zhou, <b>Teli Ma</b>, Kun-Yu Lin, Ronghe Qiu, Zifan Wang, Junwei Liang</i><br />
                        arXiv preprint, 2024 <br />
                        [<a href="https://arxiv.org/pdf/2406.14235">paper</a>][<a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">homepage</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>


            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/sigma.gif"><img src="images/sigma.gif" alt="Sigma_Agent" width="320" height="180"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation <br />
                        <i><b>Teli Ma</b>, Jiaming Zhou, Zifan Wang, Ronghe Qiu, Junwei Liang</i><br />
                        Conference on Robot Learning (<b>CoRL</b>), 2024 <br />
                        [<a href="https://arxiv.org/pdf/2406.09738">paper</a>][<a href="projects/Sigma_Agent/index.html">homepage</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>
          
            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/sade.png"><img src="images/sade.png" alt="SADE" width="300" height="200"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> An Examination of the Compositionality of Large Generative Vision-Language Models <br />
                        <i><b>Teli Ma</b>, Rong Li, Junwei Liang</i><br />
                        The North American Chapter of the Association for Computational Linguistics (<b>NAACL</b>), 2024 <br />
                        [<a href="https://arxiv.org/pdf/2308.10509.pdf">paper</a>][<a href="https://github.com/TeleeMa/SADE">code</a>][<a href="projects/SADE/sade.html">homepage</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>


            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/SyncTrack.png"><img src="images/SyncTrack.png" alt="SyncTrack" width="320" height="160"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking <br />
                        <i><b>Teli Ma*</b>, Mengmeng Wang*, Jimin Xiao, Huifeng Wu, Yong Liu</i><br />
                        Proceedings of the IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023 <br />
                        [<a href="https://arxiv.org/pdf/2308.12549.pdf">paper</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>

            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/CorpNet.png"><img src="images/CorpNet.png" alt="CorpNet" width="280" height="200"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> Correlation Pyramid Network for 3D Single Object Tracking <br />
                        <i>Mengmeng Wang, <b>Teli Ma</b>, Xingxing Zuo, Jiajun Lv, Yong Liu</i><br />
                        Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Workshops (<b>CVPRW</b>), 2023 <br />
                        [<a href="https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Wang_Correlation_Pyramid_Network_for_3D_Single_Object_Tracking_CVPRW_2023_paper.pdf">paper</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>

            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/ReActNet.png"><img src="images/ReActNet.png" alt="ReActNet" width="320" height="140"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> Resilient Binary Neural Network <br />
                        <i>Sheng Xu*, Yanjing Li*, <b>Teli Ma*</b>, Mingbao Lin, Hao Dong, Baochang Zhang, Peng Gao, Jinhu Lv</i><br />
                        Proceedings of the AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2023 <br />
                        [<font color="red">oral presentation</font>]
                        [<a href="https://arxiv.org/pdf/2302.00956.pdf">paper</a>]
                        [<a href="https://github.com/SteveTsui/ReBNN">code</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>

            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/ballad.png"><img src="images/ballad.png" alt="BALLAD" width="320" height="160"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> Unleashing the Potential of Vision-Language Models for Long-Tailed Visual Recognition <br />
                        <i><b>Teli Ma</b>, Shijie Geng, Mengmeng Wang, Sheng Xu, Hongsheng Li, Baochang Zhang, Peng Gao, Yu Qiao</i><br />
                        The British Machine Vision Conference (<b>BMVC</b>), 2022 <br />
                        [<a href="https://bmvc2022.mpi-inf.mpg.de/0481.pdf">paper</a>]
                        [<a href="https://github.com/TeleeMa/BALLAD">code</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>

            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/mcmae.png"><img src="images/mcmae.png" alt="MCMAE" width="300" height="200"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> MCMAE: Masked Convolution Meets Masked Autoencoders <br />
                        <i>Peng Gao, <b>Teli Ma</b>, Hongsheng Li, Ziyi Lin, Jifeng Dai, Yu Qiao</i><br />
                        Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2022 <br />
                        [<font color="red">spotlight</font>]
                        [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/e7938ede51225b490bb69f7b361a9259-Paper-Conference.pdf">paper</a>]
                        [<a href="https://github.com/Alpha-VL/ConvMAE">code</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>

            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/ida_det.png"><img src="images/ida_det.png" alt="IDa-Det" width="320" height="180"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> IDa-Det: An Information Discrepancy-aware Distillation for 1-bit Detectors <br />
                        <i>Sheng Xu*, Yanjing Li*, Bohan Zeng*, <b>Teli Ma</b>, Baochang Zhang, Xianbin Cao, Peng Gao, Jinhu Lv</i><br />
                        European Conference on Computer Vision (<b>ECCV</b>), 2022 <br />
                        [<a href="https://link.springer.com/chapter/10.1007/978-3-031-20083-0_21">paper</a>]
                        [<a href="https://github.com/SteveTsui/IDa-Det">code</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>


            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/rebnn.png"><img src="images/rebnn.png" alt="ReBNN" width="320" height="180"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> Recurrent Bilinear Optimization for Binary Neural Networks <br />
                        <i>Sheng Xu*, Yanjing Li*, Tiancheng Wang, <b>Teli Ma</b>, Baochang Zhang, Peng Gao, Yu Qiao, Jinhu Lv, Guodong Guo</i><br />
                        European Conference on Computer Vision (<b>ECCV</b>), 2022 <br />
                        [<font color="red">oral presentation</font>]
                        [<a href="https://link.springer.com/chapter/10.1007/978-3-031-20053-3_2">paper</a>]
                        [<a href="https://github.com/SteveTsui/RBONN">code</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>

            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/dsnet.png"><img src="images/dsnet.png" alt="DSNet" width="320" height="200"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> Dual-stream network for visual recognition <br />
                        <i>Mingyuan Mao*, Renrui Zhang*, Honghui Zheng*, <b>Teli Ma</b>, Yan Peng, Errui Ding, Baochang Zhang, Shumin Han</i><br />
                        Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2021 <br />
                        [<a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf">paper</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>

            <tr> 
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <a href="images/clip_adapter.png"><img src="images/clip_adapter.png" alt="CLIP-Adapter" width="300" height="200"></a>
                </td>
                <td width="65%" valign="middle">
                    <p> Clip-adapter: Better vision-language models with feature adapters <br />
                        <i>Peng Gao*, Shijie Geng*, Renrui Zhang*, <b>Teli Ma</b>, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, Yu Qiao</i><br />
                        International Journal of Computer Vision (<b>IJCV</b>), 2023 <br />
                        [<a href="https://arxiv.org/pdf/2110.04544.pdf">paper</a>]
                        [<a href="https://github.com/gaopengcuhk/CLIP-Adapter">code</a>]<br />
                        </p>
                        <br />
                      
                </td>
            </tr>
        
        </tbody></table>

				

					
      </td>
    </tr>
  </table>
</body>

</html>