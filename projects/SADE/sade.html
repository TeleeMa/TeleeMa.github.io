<!DOCTYPE html
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>SADE - Project Page</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="MODE">
    <!-- <meta name="robots" content="index,follow"> -->
    <link rel="author" href="https://teleema.github.io/">

    <!-- Fonts and stuff -->
    <link rel="stylesheet" type="text/css" media="screen" href="project.css">

    <!-- 
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/charts.css/dist/charts.min.css"> -->


</head>

<body>
    <div id="content">
        <div id="content-inner">

            <div class="section head">
                <h1>An Examination of the Compositionality of Large Generative Vision-Language Models</h1>

                <div class="authors">
                    <a href="https://teleema.github.io/">Teli Ma<sup>1</sup></a>&nbsp;&nbsp;
                    <a href="https://rongli.tech/">Rong Li<sup>1</sup></a>&nbsp;&nbsp;
                    <a href="https://junweiliang.me/index.html">Junwei Liang<sup>1,2</sup></a>&nbsp;&nbsp;
                </div>

                <div class="affiliations">
                    <a
                        href="https://www.hkust-gz.edu.cn/academics/hubs-and-thrust-areas/information-hub/artificial-intelligence/"><sup>1</sup>AI
                        Thrust, HKUST(GZ)</a>&nbsp;&nbsp;
                    <a href="https://cse.hkust.edu.hk/"><sup>2</sup>CSE, HKUST</a>
                </div>

                <div class="venue">NAACL
                    2024
                </div>
            </div>

            <br>

            <!-- <center><img src="./xx/intro.png" border="0" width="90%"></center> -->
            <!-- <hr> -->
            <!-- 分割线 -->
            <div class="section abstract">
                <h2>ABSTRACT</h2>
                <br>
                <p>
                With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics ( VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely SyntActically DE-biased benchmark (SADE). Our study provides an unbiased benchmark for the compositionality of GVLMs, facilitating future research in this direction. 
                </p>
            </div>

            <br>

            <hr>
            <!-- 分割线 -->
            <div class="section materials">
                <h2>RESOURCES</h2>
                <center>
                    <ul>
                        <li class="grid">
                            <div class="griditem">
                                <a href="https://arxiv.org/abs/2308.10509" target="_blank" class="imageLink">
                                    <img src="arxiv.png">
                                </a><br>
                                <a href="https://arxiv.org/abs/2308.10509" target="_blank">Paper</a>
                            </div>
                        </li>
                        <li class="grid">
                            <div class="griditem">
                                <a href="https://github.com/TeleeMa/MODE" target="_blank" class="imageLink">
                                    <img src="github.png">
                                </a><br>
                                <a href="https://github.com/TeleeMa/MODE" target="_blank">Code</a>
                            </div>
                        </li>
                        <!-- <li class="grid">
                            <div class="griditem">
                                <a href="https://xxx" target="_blank" class="imageLink"><img
                                        src="./xxx/xxx.jpg"></a><br>
                                <a href="https://xxx" target="_blank">Extended Paper</a>
                            </div>
                        </li> -->

                    </ul>
                </center>
            </div>
            <br>

            <hr>
            <!-- 分割线 -->
            <div class="section presentation">
                <h2>HIGHLIGHTS</h2>
                <br>
                <center>

                    <div class="highlights-content">
                        <!-- <div style="width:100%; text-align: center;"> -->
                        <div style="width:100%; text-align: left;">
                            <h3>* A prevalent syntactical bias is present in contemporary multimodal compositional reasoning benchmarks.These benchmarks are tailored for assessing EVLMs, and the approach
                                used to create negative references may not be effective for the evaluation of GVLMs.
                            </h3>
                            <!-- <br> -->
                            <!-- <br> -->
                            <img style="width:100%" src="fig1.png" alt='fig1'>
                        
                            <img style="width:100%" src="fig2.png" alt='fig2'>
                        </div>
                    </div>

                    <br>
                    <br>



                        <div class="highlights-content">
                            <div style="width:100%; text-align: left;">
                                <h3>* We quantitatively analyze the syntactical bias
                                    (namely SyntaxBias Score) that broadly exists
                                    in current benchmarks by leveraging LLMs.</h3>
                                <!-- <br>
                                <br> -->
                                <div align="center">
                                    <img style="width:80%" src="fig3.png" alt='fig3'>
                                </div>
                            </div>

                            <br>
                            <br>


                            <div class="highlights-content">
                                <div style="width:100%; text-align: left;">
                                    <h3>* With the SyntaxBias Score, we propose a
                                        SyntActically DE-biased benchmark (SADE)
                                        based on current benchmarks for a more robust multimodal compositionality evaluation.
                                        We adopt multiple strategies to mitigate the
                                        syntactical bias in existing benchmarks. We
                                        also add a new challenging assessment in
                                        SADE to evaluate the content understanding
                                        across visual and language modalities.</h3>
                                    <!-- <br>
                                    <br> -->
                                    <div align="center">

                                        <img style="width:100%" src="fig4.png" alt='fig4'>
                                        <img style="width:100%" src="fig5.png" alt='fig5'>
                                    </div>
                                </div>
                                <br>
                                <br>


                            <div class="highlights-content">
                                    <div style="width:100%; text-align: left;">
                                        <h3>* The performance of several GVLMs is reported on SADE, as well as the robustness
                                            and faithfulness to human judgments. </h3>
                                        <!-- <br>
                                        <br> -->
                                        <div align="center">
                                            <img style="width:90%" src="fig6.png" alt='fig6'>
                                        </div>
                                    </div>
                </center>
            </div>

            <br>
            <br>
            
            <hr>
            <!-- 分割线 -->

            <div class="section citation">
                <h2>CITATION</h2>
                <div class="section bibtex">
                    <!-- <div> -->
                <pre>@article{ma2023examination, 
        title={An examination of the compositionality of large generative vision-language models},
        author={Ma, Teli and Li, Rong and Liang, Junwei},
        journal={arXiv preprint arXiv:2308.10509},
        year={2023}}                      
</pre>
                </div>
            </div>

</body>

</html>