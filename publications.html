<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="misc/UST.svg.png" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Teli Ma</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<!-- <div class="menu-item"><a href="experience.html">Experience</a></div>
<div class="menu-item"><a href="research.html">Research</a></div> -->
<!-- <div class="menu-item"><a href="awards.html">Awards</a></div> -->
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
<!-- <div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="activities.html">Activities</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>
<p>In this page, name &ldquo;Teli Ma&rdquo; is shown in bold text and symbol &ldquo;*&rdquo; denotes the equal contribution.</p>
<p>The publications can also be found through <a href="https://scholar.google.com/citations?user=arny77IAAAAJ&hl=en&oi=ao">Google Scholar</a>.</p>
<h2>Selected Preprints</h2>
<ul>
<li><p> Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation <br />
<i><b>Teli Ma</b>, Jiaming Zhou, Zifan Wang, Ronghe Qiu, Junwei Liang</i><br />
arXiv preprint, 2024 <br />
[<a href="https://arxiv.org/pdf/2406.09738">paper</a>][<a href="projects/Sigma_Agent/index.html">homepage</a>]<br />
</p>
</li>
<!-- <br /> -->
<li><p> Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation <br />
  <i>Jiaming Zhou, <b>Teli Ma</b>, Kun-Yu Lin, Ronghe Qiu, Zifan Wang, Junwei Liang</i><br />
  arXiv preprint, 2024 <br />
  [<a href="https://arxiv.org/pdf/2406.14235">paper</a>][<a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">homepage</a>]<br />
  </p>
  </li>

<!-- <li><p> Oriented object detection with transformer <br />
  <i><b>Teli Ma</b>, Mingyuan Mao, Honghui Zheng, Peng Gao, Xiaodi Wang, Shumin Han, Errui Ding, Baochang Zhang, David Doermann</i><br />
  arXiv preprint, 2021 <br />
  [<a href="https://arxiv.org/pdf/2106.03146.pdf">paper</a>]<br />
  </p>
  </li>
<br /> -->
</ul>

<h2>Selected Conference Proceedings</h2>
<ul>
<li><p> An Examination of the Compositionality of Large Generative Vision-Language Models <br />
  <i><b>Teli Ma</b>, Rong Li, Junwei Liang</i><br />
  The North American Chapter of the Association for Computational Linguistics (<b>NAACL</b>), 2024 <br />
  [<a href="https://arxiv.org/pdf/2308.10509.pdf">paper</a>][<a href="https://github.com/TeleeMa/SADE">code</a>][<a href="projects/SADE/sade.html">homepage</a>]<br />
  </p>
  </li>
<br />

<li><p> Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking <br />
<i><b>Teli Ma*</b>, Mengmeng Wang*, Jimin Xiao, Huifeng Wu, Yong Liu</i><br />
Proceedings of the IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023 <br />
[<a href="https://arxiv.org/pdf/2308.12549.pdf">paper</a>]<br />
</p>
</li>
<br />

<li><p> Correlation Pyramid Network for 3D Single Object Tracking <br />
  <i>Mengmeng Wang, <b>Teli Ma</b>, Xingxing Zuo, Jiajun Lv, Yong Liu</i><br />
  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Workshops (<b>CVPRW</b>), 2023 <br />
  [<a href="https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Wang_Correlation_Pyramid_Network_for_3D_Single_Object_Tracking_CVPRW_2023_paper.pdf">paper</a>]<br />
  </p>
  </li>
<br />

<li><p> Resilient Binary Neural Network <br />
    <i>Sheng Xu*, Yanjing Li*, <b>Teli Ma*</b>, Mingbao Lin, Hao Dong, Baochang Zhang, Peng Gao, Jinhu Lv</i><br />
    Proceedings of the AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2023 <br />
    [<font color="red">oral presentation</font>]
    [<a href="https://arxiv.org/pdf/2302.00956.pdf">paper</a>]
    [<a href="https://github.com/SteveTsui/ReBNN">code</a>]<br />
    </p>
    </li>
<br />

<li><p> Unleashing the Potential of Vision-Language Models for Long-Tailed Visual Recognition <br />
  <i><b>Teli Ma</b>, Shijie Geng, Mengmeng Wang, Sheng Xu, Hongsheng Li, Baochang Zhang, Peng Gao, Yu Qiao</i><br />
  The British Machine Vision Conference (<b>BMVC</b>), 2022 <br />
  [<a href="https://bmvc2022.mpi-inf.mpg.de/0481.pdf">paper</a>]
  [<a href="https://github.com/TeleeMa/BALLAD">code</a>]<br />
  </p>
  </li>
<br />

<li><p> MCMAE: Masked Convolution Meets Masked Autoencoders <br />
  <i>Peng Gao, <b>Teli Ma</b>, Hongsheng Li, Ziyi Lin, Jifeng Dai, Yu Qiao</i><br />
  Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2022 <br />
  [<font color="red">spotlight</font>]
  [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/e7938ede51225b490bb69f7b361a9259-Paper-Conference.pdf">paper</a>]
  [<a href="https://github.com/Alpha-VL/ConvMAE">code</a>]<br />
  </p>
  </li>
<br />

<li><p> IDa-Det: An Information Discrepancy-aware Distillation for 1-bit Detectors <br />
  <i>Sheng Xu*, Yanjing Li*, Bohan Zeng*, <b>Teli Ma</b>, Baochang Zhang, Xianbin Cao, Peng Gao, Jinhu Lv</i><br />
  European Conference on Computer Vision (<b>ECCV</b>), 2022 <br />
  [<a href="https://link.springer.com/chapter/10.1007/978-3-031-20083-0_21">paper</a>]
  [<a href="https://github.com/SteveTsui/IDa-Det">code</a>]<br />
  </p>
  </li>
<br />

<li><p> Recurrent Bilinear Optimization for Binary Neural Networks <br />
  <i>Sheng Xu*, Yanjing Li*, Tiancheng Wang, <b>Teli Ma</b>, Baochang Zhang, Peng Gao, Yu Qiao, Jinhu Lv, Guodong Guo</i><br />
  European Conference on Computer Vision (<b>ECCV</b>), 2022 <br />
  [<font color="red">oral presentation</font>]
  [<a href="https://link.springer.com/chapter/10.1007/978-3-031-20053-3_2">paper</a>]
  [<a href="https://github.com/SteveTsui/RBONN">code</a>]<br />
  </p>
  </li>
<br />

<li><p> Dual-stream network for visual recognition <br />
  <i>Mingyuan Mao*, Renrui Zhang*, Honghui Zheng*, <b>Teli Ma</b>, Yan Peng, Errui Ding, Baochang Zhang, Shumin Han</i><br />
  Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2021 <br />
  [<a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf">paper</a>]<br />
  </p>
  </li>
<!-- <br /> -->
</ul>

<h2>Selected Journal Articles</h2>
<ul>
<li><p> Clip-adapter: Better vision-language models with feature adapters <br />
<i>Peng Gao*, Shijie Geng*, Renrui Zhang*, <b>Teli Ma</b>, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, Yu Qiao</i><br />
International Journal of Computer Vision (<b>IJCV</b>), 2023 <br />
[<a href="https://arxiv.org/pdf/2110.04544.pdf">paper</a>]
[<a href="https://github.com/gaopengcuhk/CLIP-Adapter">code</a>]<br />
</p>
</li>
<br />
</ul>

</td>
</tr>
</table>
</body>
</html>
